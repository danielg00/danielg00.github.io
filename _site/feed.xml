<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.7">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2020-07-02T21:05:58+01:00</updated><id>http://localhost:4000/</id><title type="html">Daniel’s blog</title><subtitle>A blog for me to convey my ideas and to explain the ideas of others. Ideas will centre around AI but might creep into areas of metaphyics, general maths, or computer science.</subtitle><entry><title type="html">Contravariant and covariant ideation.</title><link href="http://localhost:4000/misc/2020/07/01/CovariantAndContravariantMeanings.html" rel="alternate" type="text/html" title="Contravariant and covariant ideation." /><published>2020-07-01T00:00:00+01:00</published><updated>2020-07-01T00:00:00+01:00</updated><id>http://localhost:4000/misc/2020/07/01/CovariantAndContravariantMeanings</id><content type="html" xml:base="http://localhost:4000/misc/2020/07/01/CovariantAndContravariantMeanings.html">&lt;p&gt;One of Wittgenstein’s more notable proposals was that language doesn’t play only a passive role in our conceptualis ation of the world, but also an active one. One can see this 
by examining the process of ideation. We:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Are given some form of sensory input and told that label/word etc X is given to this input,&lt;/li&gt;
  &lt;li&gt;We aggregate this over time to get a more general idea of what X is,&lt;/li&gt;
  &lt;li&gt;We then project that perception onto the world to find out what we can call “X”.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In linear algebra, or more generally tensor analysis, we have so called &lt;a href=&quot;https://en.wikipedia.org/wiki/Covariant_transformation&quot;&gt;&lt;em&gt;covariant&lt;/em&gt; and &lt;em&gt;contravariant&lt;/em&gt;&lt;/a&gt; co-ordinate transformations. In layman’s terms: say we use some measure or scale
to measure a distance. A covariant transformation is one that directly distorts (shrinks or enlarges) the distance we are measuring. Conversely, a
contravariant transformation is one that distorts the measurement we are using, therefore giving the perception that the distance is changing.&lt;/p&gt;

&lt;p&gt;Succinctly, one may think of &lt;em&gt;semantic&lt;/em&gt; changes of a word as covariant transformations (same word, different concept) and &lt;em&gt;syntactic&lt;/em&gt; changes as contravariant 
transformations (different word, same concept).&lt;/p&gt;

&lt;p&gt;A concrete example of a covariant transformation are “stars”. The underlying meaning of what we denote with the word &lt;a href=&quot;https://en.wikipedia.org/wiki/History_of_astronomy#Early_history&quot;&gt;“star”&lt;/a&gt; has drastically changed over the course of time. Whatever the ancients thought stars were (gods, cracks in the sky to heaven, more Suns) they did not know stars were massive balls of predominantly hydrogen and helium, powered by nuclear fusion due to huge pressures at its core.&lt;/p&gt;

&lt;p&gt;Many of the politics discussed today are a result of this antagonism, and many falsely put syntax before semantics. I have no doubt that I’ve given a superficial treatment of the idea, but it is any interesting one nonetheless.&lt;/p&gt;</content><author><name></name></author><summary type="html">One of Wittgenstein’s more notable proposals was that language doesn’t play only a passive role in our conceptualis ation of the world, but also an active one. One can see this by examining the process of ideation. We:</summary></entry><entry><title type="html">A quick catch-up on Disentanglement Learning.</title><link href="http://localhost:4000/ai/2020/06/30/disentanglementlearing.html" rel="alternate" type="text/html" title="A quick catch-up on Disentanglement Learning." /><published>2020-06-30T00:00:00+01:00</published><updated>2020-06-30T00:00:00+01:00</updated><id>http://localhost:4000/ai/2020/06/30/disentanglementlearing</id><content type="html" xml:base="http://localhost:4000/ai/2020/06/30/disentanglementlearing.html">&lt;h3 id=&quot;preamble&quot;&gt;Preamble&lt;/h3&gt;
&lt;p&gt;Disentanglement learning aims to learning features or representations that are independent of their embedded context. A concrete example of where disentanglement learning becomes useful is for basic 
extrapolation tasks. For instance, show an observer a scene of a basketball on a wall and give it a label. Then show it a bottle on its own or in some other context and give it another label. If we’ve learned 
a disentangled representation of these two inputs, the observer should be able to deduce the label for a scene with a bottle on a wall. It sounds painstakingly simple, but nonetheless an example of emergent extrapolation, in a sense. Arguably it forms the basis for simple extrapolation.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1301.3781&quot;&gt;[1]&lt;/a&gt; does something similar in their paper on Word2Vec, demonstrating how the embedding space can &lt;a href=&quot;https://towardsdatascience.com/how-to-solve-analogies-with-word2vec-6ebaf2354009?gi=5419100a6e07&quot;&gt;model analogies&lt;/a&gt;. For instance, if one wished to solve the analogy “Man is to Woman, as King is to ?(Queen)”. One can 
get the word vectors $v_{man}, v_{woman}, v_{king}$, and see that $v_{king} - v_{man} + v_{woman} \approx v_{queen}$. Assuming that each word has some feature such as “royalty” and “gender”, one can see
how this is an “analogy” for disentanglement learning.&lt;/p&gt;

&lt;p&gt;The explanation of the task given &lt;a href=&quot;https://deepai.org/machine-learning-glossary-and-terms/disentangled-representation-learning&quot;&gt;here&lt;/a&gt; on deepai.org feels vague:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Disentangled representation is an unsupervised learning technique that breaks down, or disentangles, each feature into narrowly defined variables and encodes them as separate dimensions. 	The goal is to mimic the quick intuition process of a human, using both “high” and “low” dimension reasoning. For example, in a predictive network processing images of people, “higher dimensional”
features such as height and clothing would be used to determine sex. In a generative network version of that model designed to produce images of people from a stock photo database, these would be 
broken down into separate, lower dimensional features. Such as: total height of each person, length of arms and legs, type of shirt, type of pants, type of shoe, etc…&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Indeed, we see that perhaps the notion of disentanglement learning is ill-posed in the unsupervised setting; perhaps it’s not the notion of learning disjoint representations that is needed but the notion
of discovering what features deterministically induce labels/outputs. This seems like prime territory for &lt;a href=&quot;https://arxiv.org/abs/1812.08434&quot;&gt;GNNs&lt;/a&gt; and I’ll be writing more on it at a later date.&lt;/p&gt;

&lt;h3 id=&quot;current-research&quot;&gt;Current Research&lt;/h3&gt;
&lt;p&gt;The current research around disentanglement learning seems underwhelming, and the few theoretical results don’t yield promising conclusions! &lt;a href=&quot;https://arxiv.org/abs/1811.12359&quot;&gt;[2]&lt;/a&gt; shows that many of the models
tailored towards disentangled learning in unsupervised settings are deficient&lt;/p&gt;

&lt;p&gt;&lt;em&gt;We first theoretically show that the unsupervised learning of disentangled representations is fundamentally impossible without inductive biases on both the models and the data. […] 
	Then, we train more than 12000 models covering most prominent methods and evaluation metrics in a reproducible large-scale experimental study on seven different data sets. 
We observe that while the different methods successfully enforce properties “encouraged” by the corresponding losses, well-disentangled models seemingly cannot be identified without supervision.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://papers.nips.cc/paper/7333-learning-to-decompose-and-disentangle-representations-for-video-prediction.pdf&quot;&gt;[3]&lt;/a&gt; is a nice paper where the “label” for a video frame can be interpreted as the subsequent
video frame in the task of predicting videos by disentangling video frames.&lt;/p&gt;

&lt;p&gt;An obvious(if not the biggest) problem is that disentanglement learning must implicitly solve the problem of recognizing objects under transformations (i.e. rotations, translations, etc.). 
Deepmind’s paper &lt;a href=&quot;https://deepmind.com/research/publications/towards-definition-disentangled-representations&quot;&gt;[4]&lt;/a&gt; attempts to define the task under this scheme. To quote:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Here we propose that a principled solution to characterising disentangled representations can be found by focusing on the transformation properties of the world.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;They propose that a group $G$ acts on a space $X$ through $\cdot:G \times X \rightarrow X$, and that $G$ decomposes into the product of subgroups $G = G_1 \times G_2 \times … $. Perhaps one interesting 
generalisation of this idea is that $G$ acts on the powerset $\mathcal{P}(X)$. That is, the map may not be bijective and that certain features “collapse”. One could leverage this to explain (or
devise) algorithms in the supervised setting - particularly maps to one-hot labels.&lt;/p&gt;

&lt;p&gt;To be continued…&lt;/p&gt;</content><author><name></name></author><summary type="html">Preamble Disentanglement learning aims to learning features or representations that are independent of their embedded context. A concrete example of where disentanglement learning becomes useful is for basic extrapolation tasks. For instance, show an observer a scene of a basketball on a wall and give it a label. Then show it a bottle on its own or in some other context and give it another label. If we’ve learned a disentangled representation of these two inputs, the observer should be able to deduce the label for a scene with a bottle on a wall. It sounds painstakingly simple, but nonetheless an example of emergent extrapolation, in a sense. Arguably it forms the basis for simple extrapolation.</summary></entry><entry><title type="html">Physics Introduction</title><link href="http://localhost:4000/physics/2020/06/29/PhysicsPlans.html" rel="alternate" type="text/html" title="Physics Introduction" /><published>2020-06-29T00:00:00+01:00</published><updated>2020-06-29T00:00:00+01:00</updated><id>http://localhost:4000/physics/2020/06/29/PhysicsPlans</id><content type="html" xml:base="http://localhost:4000/physics/2020/06/29/PhysicsPlans.html">&lt;p&gt;My goal for physics is to catch up on constructive quantum field theory. Because of this, a lot of the writing here will be expositions on prerequisite topics such as QM, GR, Yang-Mils
theory, TQFT etc. The mathematical prerequisites for those will be in the math section.&lt;/p&gt;</content><author><name></name></author><summary type="html">My goal for physics is to catch up on constructive quantum field theory. Because of this, a lot of the writing here will be expositions on prerequisite topics such as QM, GR, Yang-Mils theory, TQFT etc. The mathematical prerequisites for those will be in the math section.</summary></entry><entry><title type="html">Table of Contents for Intro to Homology Theory</title><link href="http://localhost:4000/math/2020/06/29/IntroToHomDoc.html" rel="alternate" type="text/html" title="Table of Contents for Intro to Homology Theory" /><published>2020-06-29T00:00:00+01:00</published><updated>2020-06-29T00:00:00+01:00</updated><id>http://localhost:4000/math/2020/06/29/IntroToHomDoc</id><content type="html" xml:base="http://localhost:4000/math/2020/06/29/IntroToHomDoc.html">&lt;p&gt;On of the first docs I intend to upload is a PDF introducing (co)homology theory that eventually builds up and intuition better that “it counts the amount of n dimensional holes”. The tentative table of contents can be found &lt;a href=&quot;http://localhost:4000/assets/introtohomtheory.pdf&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The abstract reads: 
	In this paper we introduce the reader to the theory of homology. In Part 1 we begin by covering the simple case of simplicial homology. We then seek generalisations and variants,
	and eventually formulate a more general notion of homology through category theory in Part 2. Finally, we use this more general notion to sample other forms of homology that arise in topics such as
	physics, algebraic geometry, and data science.&lt;/p&gt;</content><author><name></name></author><summary type="html">On of the first docs I intend to upload is a PDF introducing (co)homology theory that eventually builds up and intuition better that “it counts the amount of n dimensional holes”. The tentative table of contents can be found here.</summary></entry><entry><title type="html">Misc Introduction</title><link href="http://localhost:4000/misc/2020/06/29/AboutMisc.html" rel="alternate" type="text/html" title="Misc Introduction" /><published>2020-06-29T00:00:00+01:00</published><updated>2020-06-29T00:00:00+01:00</updated><id>http://localhost:4000/misc/2020/06/29/AboutMisc</id><content type="html" xml:base="http://localhost:4000/misc/2020/06/29/AboutMisc.html">&lt;p&gt;This section will be dedicated to interesting links, writing about more esoteric sections of stem, the occasional political comment, esoteric words etc.&lt;/p&gt;</content><author><name></name></author><summary type="html">This section will be dedicated to interesting links, writing about more esoteric sections of stem, the occasional political comment, esoteric words etc.</summary></entry><entry><title type="html">Math Introduction</title><link href="http://localhost:4000/math/2020/06/29/AboutMath.html" rel="alternate" type="text/html" title="Math Introduction" /><published>2020-06-29T00:00:00+01:00</published><updated>2020-06-29T00:00:00+01:00</updated><id>http://localhost:4000/math/2020/06/29/AboutMath</id><content type="html" xml:base="http://localhost:4000/math/2020/06/29/AboutMath.html">&lt;p&gt;My main interests in maths currently are topics in topology in geometry. Relevant books I am reading include Bredon’s “Geometry and Topology”, Lang’s “Algebra”, and Nakahara’s 
“Geometry, Topology and Physics”.&lt;/p&gt;</content><author><name></name></author><summary type="html">My main interests in maths currently are topics in topology in geometry. Relevant books I am reading include Bredon’s “Geometry and Topology”, Lang’s “Algebra”, and Nakahara’s “Geometry, Topology and Physics”.</summary></entry><entry><title type="html">AI introduction</title><link href="http://localhost:4000/ai/2020/06/29/AboutAI.html" rel="alternate" type="text/html" title="AI introduction" /><published>2020-06-29T00:00:00+01:00</published><updated>2020-06-29T00:00:00+01:00</updated><id>http://localhost:4000/ai/2020/06/29/AboutAI</id><content type="html" xml:base="http://localhost:4000/ai/2020/06/29/AboutAI.html">&lt;p&gt;My main interests AI are the potential formalisms that can help develop new techniques. The derivation of SVM from Statistical Learning Theory is a prime example of this.&lt;/p&gt;</content><author><name></name></author><summary type="html">My main interests AI are the potential formalisms that can help develop new techniques. The derivation of SVM from Statistical Learning Theory is a prime example of this.</summary></entry></feed>