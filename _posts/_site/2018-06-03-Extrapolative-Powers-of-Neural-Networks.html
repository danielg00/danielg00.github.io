<h4 id="introduction-1">Introduction</h4>
<p>Extrapolation is an integral part of intelligence.</p>

<p>Vaguely, the two main components of extrapolation are the identification of sub-symbols and the logical synthesis of such sub-symbols into a desired output/label by learning how these sub-symbols interact.
Softmax neural networks seem to pass through this regime in am approximate manner. The main body of a N.N. ‘kinda’ identifies symbols and classification via softmax function ‘kinda’ synthesize these
symbols - albeit, not well and tend to require large amounts of homogeneous data.</p>

<p>In this blog post I conduct various rudimentary experiments to test the extrapolative abilities of neural networks. The result of the experiments seem to point to towards the composition of softmax NNs lacking
the necessary characteristics to preform rudimentary extrapolation. I emphasize that this obviously isn’t justification to abandon deep learning methods, but hint towards that we may want to be more careful
on how we encode and interpret the output of a neural network.</p>

<p>Conclusively, neural networks seems specious and only suitable for interpolating. One can think of it as analogous to Monte Carlo methods: it can only draw an accurate estimate if it samples from the full
distribution of a dataset and fail to derive the underlying mechanism/logic of such datasets.</p>

<p>I will speak more on these ideas when we gain more context after the experiments.</p>

<hr />

<h4 id="data">Data.</h4>

<p>For these experiments I made modifications to the MNIST dataset. A single image will consist of two main objects (or attributes) that describe the image.</p>

<p><img src="http://localhost:4000/assets/Exp_pwrs/color_digits_50p.jpg" alt="" /></p>

<p>This is a sample of the dataset for the first experiment. As you can see it features two main attributes -the color of the digit and the digit itself.</p>

<p>Throughout the rest of the blog post I’ll denote the digit as <em>datum type 1</em> and the other attribute as <em>datum type 2</em>. Here datum type 2 would be the color of the digits.</p>

<p><img src="http://localhost:4000/assets/Exp_pwrs/cross_squares_50p.jpg" alt="" /></p>

<p>This is a sample of data from experiment two. Here datum type 2 is the object at the top left of the image {a cross, a solid box, or exemption of any visible attribute}.</p>

<h4 id="modus-operandi">Modus Operandi.</h4>

<p>Within each datum there are $n$ are sub-datums. For instance, in experiment two there are $n=3$ sub-datums making up datum type 2 ($n=10$ for datum type 1).
These constructions have no real significance and only serve as a means to make what I’m saying more concise and clear.</p>

<p>Although the experiment’s details vary, they all follow the same underlying pattern.</p>

<ol>
  <li>We have two types of datum in an image (e.g colored digits, digit with square at the top left, etc).</li>
  <li>We generate a combination of datum types, $D_E$, and exclude them from the training process (e.g all the red sevens, green ones etc. We make sure not to exclude full classes of datums such as all the
red digits etc.)</li>
  <li>After training on the remaining samples, $D_R$, we test the network on the excluded pairs.</li>
</ol>

<p>If a model were to successfully classify these excluded pairs it would mean it would be internally symbolizing each datum type and synthesizing these symbols into a label.
This idea is congruent to vanilla classification- we have combinations of features(a curve, straight line, shade, etc.) that a label logically arises from. It stress “logically arises from”- what combination of 
features leads to a label should be deteminsitic. Logical deduction, in the exterapolative sense, is much less a requisite in generic classification tasks(MNIST, CIFAR etc.) where classifying is much more analogous to fitting a learned template; we must see the whole set of features pertaining to each class to accurately make predictions. Conversely, real-life situations do not grant us these niceties. Reality requires
us to splice and logically synthesize features from different aspects of our experience, creating something like an acyclic graph.</p>

<h5 id="encoding">Encoding.</h5>

<p>I use two types of encoding:</p>

<p><strong>Softmax Encoding:</strong></p>

<p>To find the class of an image for a one-hot vector we can use the formula</p>

<p>$ i = N_k + M_j  M_n $</p>

<p>Here $N_k$, $M_j$ and $M_n$ is the $k^{th}$ datum type 1, the $j^{th}$ datum type 2 and the total, $n$, possible datum types of datum type 1, respectively. 
The order of the datum types can arbitrary.</p>

<p><strong>Regressive-like outputs:</strong></p>

<p>$ O = [M_j, N_k]$</p>

<p>Here I just concatenate labels for the two data types together giving us a two dimensional ordinal output. An output is correct if each respective entry is within 0.5 of the entries in the label.</p>

<p>This is is perhaps the most basic but preforms badly on even basic, non-extrapolative classification.</p>

<p>Although completely unusable in real world settings, it makes up what for interpretability for what it lacks in performance.</p>

<h5 id="model-and-training">Model and training.</h5>

<p>The network goes as follows:</p>

<p>Conv2d((f=16,$k=[2\times 2]$,s=2)) $\rightarrow$ BatchNorm1d $\rightarrow$ Maxpool&amp;ReLU(($k=[2\times 2]$))</p>

<p>$\rightarrow$ Conv2d((f=32,$k=[2\times 2]$,s=1)) $\rightarrow$ BatchNorm1d $\rightarrow$ Maxpool&amp;ReLU(($k=[2\times 2]$))</p>

<p>$\rightarrow$ Fully Connected layers dependent on output method.</p>

<p>Where (f), (k), and (s) are the number of convolutions, their kernel size, and their stride, respectively.</p>

<p>I used the same model throughout the experiment. The focus of these experiment was not to achieve state-of-the-art accuracy but to demonstrate extrapolative power,
therefore the model I defined was quite basic. As you can see in the results below, it has quite paltry performance on MNIST.</p>

<p>I trained the model with RMSprop for 1500 iterations with a learning rate of 0.001 and batch size of 128.</p>

<p>Little of this matters, I’m just trying to fill what would have been an empty column with text.</p>

<p>Experiment #1.  #Note to self, Should touch on consequences on R.L .</p>

<p>The first experiment I conducted was on a colored version of MNIST.</p>

<h4 id="results">Results</h4>
<p>Colored MNSIT digits</p>

<p>Here the two categories are color (red, green, blue) and digit (0, …, 9).</p>

<p>The network preforms well on the original task but preforms poorly (0% accuracy) when it comes to extrapolation.</p>

<p>exper1</p>

<p>Where A, B, C are the accuracy on the trained data, the accuracy on the the excluded data, and the decrease of accuracy between the two respectively.</p>

<p>I will comment on all the results later.
Experiment #2.</p>

<p>The second is an experiment more apt for convolutions. Instead of having to realize colors the model must simply recognize either solid square in the top left. As you can see, we’ve also reverted back to the black and white, 1 channel images.</p>

<p>crosses_squares</p>

<hr />

<h2 id="discussion">Discussion.</h2>
<p>Regressive output.</p>

<p>As stated, the regressive outputs provide little real-world utility but yield interesting heuristic information. For instance, in experiment #1 it succeeds in classifying the digit correctly but fails in recognizing it’s color.</p>

<p>In experiment #2 we observe the the converse. The model does worse when recognizing digits but classifies the second data type in the image better. Digits are ~5% better than random choice, and shapes ~ 27% better that random choice.
Softmax output.</p>

<p>As you might of realized, softmax does worse than randomly guessing. Not only this, but it as training progresses its accuracy become worse. This could be analogous to the concept of over-fitting. As the network fits for more data in input space it does this as the expense of certain other sectors in the form of assigning it to certain labels.</p>

<p>Ultimately, it is clear that
Visualization.</p>

<p>Using PCA we can see the input space being laid out as expected. There are three stratifications accounting for the variability of the data containing a box/cross/or nothing and ten amorphous stratifications orthogonal to the three.</p>

<!-- <div> -->
<!--     <a href="https://plot.ly/~danielg00/6/?share_key=xnkXvAYfHGSKGoQ5GnElQi" target="_blank" title="plot.html" style="display: block; text-align: center;"> -->
<!-- 	<img src="https://plot.ly/~danielg00/6.png?share_key=xnkXvAYfHGSKGoQ5GnElQi" alt="plot.html" style="max-width: 100%;width: 600px;" -->
<!-- 		width="600" onerror="this.onerror=null;this.src='https://plot.ly/404.png';" /></a> -->
<!--     <script data-plotly="danielg00:6" sharekey-plotly="xnkXvAYfHGSKGoQ5GnElQi" src="https://plot.ly/embed.js" async></script> -->
<!-- </div> -->

<p>Conclusion.</p>

<!-- <img src="http://localhost:4000/assets/Exp_pwrs/Figure_1.png" alt="drawing" width="900px"/> -->

<p>you see.</p>

<p><img src="http://localhost:4000/assets/Exp_pwrs/lat_view_comb.png" alt="drawing" width="800px" /></p>

<p>Given the nature of the input space, it seems that the softmax function and using logits makes neural networks ill-equipped to deal with. It will only ever fit data onto what it sees, and
Need to expand on … Plot embeddings of output of trained network</p>

<p>TODO:
    Create table of results _/
    Links to reasoning.
    Whats needed to implement this.
    Transfer learning.
    Reinforcement learning.
    Geometric explanation of what happening.
    Could compare spaces of model trained with no exclusion and model.</p>
