---
layout: post
title: On the Extrapolative Powers of Neural Networks v2.
date: 2018-06-03
comments: true
external-url:
categories: AI
---
On the Extrapolative Powers of Neural Networks v2.

##### **Part I- Data that requires extrapolation.**
Extrapolation is an integral part of intelligence and is a necessity if AI as field wants to progress.

Vaguely, the two main components of extrapolation are the identification of sub-symbols and the logical synthesis of such sub-symbols into a desired output/label by learning how these sub-symbols interact.
Softmax neural networks seem to pass through this regime in an approximate manner. The main body of an N.N. has been proven itself worthy as an identification mechanism (now achieving level comparable or above
human).

What's more, the synthesis of symbols necessitates the inclusion of assumptions to reach conclusions. Preferably, these assumption should obey Ockham's Razor- that when constructing a hypothesis, one should
choose the one the requires the least amount of logical leaps to fit the data. How we represent a logical leap is a matter I'll address later in the post. For now, we can create a mock problem to better
investigate these ideas.

Like most primeval ideas in machine learning, we start by exploiting the MNIST dataset. Like I said, extrapolation involves the synthesis of symbols and what better way to manifest this necessity by using
bi-categorical data.

<img src="{{site.url}}/assets/Exp_pwrs/data2.png" alt="Image not loaded" width="100%"/>

In this type of data we have two categories each containing 10 different symbols(digits=$D_1$) plus another feature(shape in top left or color of the digit = $D_2$) consisting of 3 symbols, giving
us a total of 10x3=30 extrapolations needed to be made.

This dataset mimics real life data but on a more infantile level. One can think of the constituents of $D_1$ and $D_2$ as any arbitrary feature in a dataset, whether is be a line at a certain angle,
shade, color etc. Therefore, the model must learn how to sum these symbols/features into the desired label. Instead of using micro-features such as single line, we use macro-features such as whole digits
or certain shapes.

What is different though in this post, is the task at hand. In vanilla classification tasks, the model is allowed to see all combinations of features and can only accurately infer from new samples whose
combination of features have previously been seen. In reality, this is not the case. Consider a reinforcement learning situation. The agent, is driving through a race-course with multiple obstacles in its path.
In encounter 1, they agent is driving straight and encounters a boulder; it learns is must veer to the left to avoid the boulder and continue straight to avoid crashing into the course's barriers. In encounter 2 the agent encounters a simple bend turn left; it learns that is must steer to the left to prevent a crash into the course's barriers. In encounter 3, the agent encounters a bend in the road to the left whilst
simultaneously encountering the vile boulder. Traditional methods would mean we would have to crash into the boulder to learn how to avoid it later. A human on the other hand makes the simple deduction that
they must veer to the left and veer to the left a bit more to avoid the bush(I suspect this type of deductive reasoning can also be applied to micro-features but is much sublte).

This isn't at all rigorous. Perhaps in some universe, boulders on left turns disappear- eliminating the need for the agent to veer extra left. In other words, the characteristics of what concepts sum to (a label, action etc.) aren't independent and may well be inter-dependant. But the best starting point we can make it most simple one. How we progress when the most simple one fails, is again another matter I'll expound
on later.

Before we delve deeper, let us digress and naively test how DL methods do on this data. The model I used has the following specifications:

Conv2d\((f=16,$k=[2\times 2]$,s=2\)) $\rightarrow$ BatchNorm1d $\rightarrow$ Maxpool&ReLU\(($k=[2\times 2]$\))

$\rightarrow$ Conv2d\((f=32,$k=[2\times 2]$,s=1\)) $\rightarrow$ BatchNorm1d $\rightarrow$ Maxpool&ReLU\(($k=[2\times 2]$\))

$\rightarrow$ Fully Connected layers dependent on output method.

Where f, k, and s are the number of convolutions, their kernel size, and their stride, respectively.
I encoded the outputs by numbering $D_1$ from $[0, 9]$ and $D_2$ from $[0, 2]$, then index for the one-hot output is calculated by

$$ i = k + (j \times 10)$$

where $k$ is the $k^{th}$ symbol in $D_1$ and $j$ is the $j^{th}$ symbol in $D_2$.

Fairly generic, but suitable considering the non-complex nature of the dataset.
The extrapolative testing part uses the following modus operandi:
1. We have two types of categories in the dataset \(e.g colored digits, digit with square at the top left\).
2. We generate a combination of symbols from each category, $D_E$, and exclude them completely from the training process (e.g all the red sevens, green ones etc. We make sure not to exclude a single symbol
completely from the training process i.e. all the red digits or all the digits nines).
3. After training on the remaining samples, $D_R$, we test the network on the excluded pairs.

For now on, I'll be using the dataset with shape in the top-left corner because it maintains the sense of 'symbols' more explicitly that the colored set.

To get a sense of how the dataset is distributed in input space can use PCA.

<center><img src="{{site.url}}/assets/Exp_pwrs/Figure_1.png" alt="Image not loaded" width="95%"/></center>

<center><img src="{{site.url}}/assets/Exp_pwrs/Figure_2.png" alt="Image not loaded" width="95%"/></center>

As you can see the data replicates itself in 3 vector subspaces in input space. One would expect such clearly defined characteristics in a simple dataset like this. Perhaps this sense of replication
persists in a more complicated manner for typical data we train on, which could be regarded as a catalyst for abstraction.

As one might expect, the model does terrible. I even have a table to prove it!


<center><img src="{{site.url}}/assets/Exp_pwrs/table1_edit.png" alt="Image not loaded" width="50%"/></center>


Interestingly, it preforms worse than random guessing. Looking at the prediction, the network generally classifies one of the symbols correctly but always fails to combine the two.
This most most likely due to the absence of an output space for certain labels, and hence 'sorting' the inputs into similar spaces the network derived from what it saw.

One would not expect a naive implementation like this to work at all, but as I said, it mirrors problems we encounter where we do in-fact implement these kind of architectures.

Let's steer away from concrete examples and think in the more abstract.

##### **Part II- Entailments, Causality and Simplifying Assumptions.**

There are dozens of ways one can semantically rephrase classification as a task. One way that helps in this situation takes complex systems approach is defining it as the ***interaction*** of a set of
symbols that give rise to or ***entail*** another set of symbols(a label in this case). Symbols in this context can be single pixels or(like in the example above) whole digits, shapes etc.I stress the usage of
entailment in this fashion because I implies the notion of a kind of logical determinism- that if we had enough manpower we could create a long list of rules that would accurately classify a dataset.

Some mathematical quantification of what I'm describing would bring us a long way. To represent these ideas, \*ahem\*, symbolically I'll use

$$ I(X, Y) = Z $$

to denote that $Z$ arises from or entails the interaction, $I$ of $X \text{ and } Y$. When I use $I$ consistently throughout the post, it merely serves to denote its argument is transforming- not that it is 
transforming under a specific function $I$. In reality, $X$ and $Y$ are interdependent and functions of each other. I originally planned to denote this with $\leftrightarrow$, but started to look 
along the lines of a crank's work.

To denote that a symbol is actually a composition of symbols, $x$ and $y$, we simply append these sub-symbol's names beside each other i.e. $xy$.

Some observations...

We might observe that a symbol, $X$ interacting with nothing, $\cdot$, entails another symbol $x$:

$$I(X, \cdot) = x$$

We might also observe an instance where:

$$I(X, \cdot) = x \text{ and } I(Y, \cdot) = y$$

and that

$$I(X, Y) = xy$$

We'll specify this kind of arrangement as a simple entailment. Visually, this can be represented like

<center> <img src="{{site.url}}/assets/Exp_pwrs/diagram1.png" alt="Image not loaded" width="52%"/> </center>

Simple entailments may be rare in reality. Perhaps something inherent to $X$ triggers a different mechanism in $Y$. 

$$ I(X, Y) = Z; Z \neq xy $$

How should we go about quantifying this? 
To amend this issue we could introduce an intermediary variable, $\alpha$, so that 

$$ I(X, Y) = xy \\ 
I(xy, \alpha) = Z $$


<center> <img src="{{site.url}}/assets/Exp_pwrs/diagram2.png" alt="Image not loaded" width="45%"/> </center>

The reader might spot that $\alpha$ is congruent to the what an assumption represents. The nature of $\alpha$ is no different from symbols that model is trained on, and is a function of $I$. 
To further explain what I mean by this we need to further explore how to derive $I$.

Classical machine learning aims



